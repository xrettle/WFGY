<!-- QUESTION_ID: TU-Q005 -->
# Q005 · abc conjecture

## 0. Header metadata

```txt
ID: Q005
Code: BH_MATH_ABC_L3_005
Domain: Mathematics
Family: Number theory (Diophantine)
Rank: S
Projection_dominance: I
Field_type: analytic_field
Tension_type: consistency_tension
Status: Open
Semantics: hybrid
E_level: E1
N_level: N2
Last_updated: 2026-01-30
```

---

## 0. Effective layer disclaimer

All statements in this entry are made strictly at the effective layer of the Tension Universe (TU) framework:

* We only specify observables, summaries, mismatch fields, tension functionals, counterfactual “worlds”, and engineering patterns that operate on abc related integer triples and their statistics.
* We do not specify any underlying TU axiom system, deep generative rules, or constructive derivations of TU itself.
* We do not provide any explicit mapping from raw arithmetic data, simulations, or code into internal TU fields; we only assume that TU compatible models exist which can reproduce the listed observables when interpreted at the effective layer.
* Nothing in this file should be read as a proof or disproof of the canonical abc conjecture. All “world T / world F” constructions are counterfactual patterns for tension analysis, not claims about the actual universe.

---

## 1. Canonical problem and status

### 1.1 Canonical statement

Let `a`, `b`, `c` be nonzero coprime integers that satisfy

```txt
a + b = c.
```

Define `rad(n)` for a nonzero integer `n` as the product of the distinct prime factors of `|n|`. For example,

* `rad(18) = 2 * 3 = 6`
* `rad(60) = 2 * 3 * 5 = 30`.

One standard form of the **abc conjecture** is:

> For every real number `eps > 0` there exists a constant `K_eps > 0` such that for all coprime nonzero integers `a`, `b`, `c` with `a + b = c`, we have
>
> ```txt
> |c| <= K_eps * rad(abc)^(1 + eps).
> ```

Intuitively:

* The triple `(a, b, c)` has a very simple additive relation.
* The radical `rad(abc)` encodes the distinct primes that appear in the product `abc`.
* The conjecture says that if the product of distinct primes is small, then `c` cannot be much larger than that product, apart from a controlled power and a constant that depends only on `eps`.

Equivalently, the conjecture says that triples where `c` is “too large” relative to `rad(abc)` should be extremely rare and quantitatively controlled.

### 1.2 Status and difficulty

The abc conjecture has been open since the early 1980s. It is central in modern Diophantine number theory because:

* It unifies and implies many deep results and conjectures about Diophantine equations.
* It gives a compact explanation of how additive and multiplicative structures in the integers constrain each other.

Many statements are known to follow from abc, including:

* Strong versions of the theorem of Roth on rational approximations.
* Effective bounds in various Diophantine problems.
* Consequences related to Szpiro type conjectures and the behavior of elliptic curves.

There has been a claimed proof using inter-universal Teichmueller theory, but this has not reached the level of broad acceptance needed for the conjecture to be regarded as settled. In this document we treat abc as an open S level problem.

### 1.3 Role in the BlackHole project

Within the BlackHole S problem collection, Q005 is:

1. The reference node for **consistency_tension** in Diophantine number theory.

   * It compares three ingredients that must cohere:

     * the additive equation `a + b = c`,
     * the multiplicative structure captured by `rad(abc)`,
     * the size or height of `c`.

2. A central hub for Diophantine geometry problems.

   * Many other conjectures become “abc corollaries” when phrased in terms of bounds on heights and radicals.
   * Q005 supplies reusable functionals for “few high quality exceptions” patterns.

3. A template for hybrid encodings.

   * The underlying objects are discrete integers and primes.
   * The effective summaries used for tension measurement are continuous quantities such as logarithmic heights and averaged or quantile based qualities.

Q005 therefore serves as the Diophantine consistency template inside the Tension Universe.

### References

1. J. H. Silverman, “The Arithmetic of Elliptic Curves”, 2nd edition, Springer, 2009. See the discussion of the abc conjecture and its relation to Diophantine equations in later chapters.
2. N. Elkies, survey articles on the abc conjecture in number theory collections, describing the conjecture, examples of high quality triples, and consequences for Diophantine problems.
3. A standard text on Diophantine geometry and arithmetic, containing a dedicated section on the abc conjecture and its implications for heights and radical inequalities.
4. An encyclopedia style entry on “abc conjecture” in an authoritative mathematics reference, giving the canonical formulation and basic history.

---

## 2. Position in the BlackHole graph

This block records how Q005 sits inside the BlackHole graph among Q001 to Q125. Every edge has a one line reason that points to a concrete component or tension type.

### 2.1 Upstream problems

These nodes provide prerequisites and conceptual tools that Q005 relies on at the effective layer.

* Q016 (BH_MATH_ZFC_CH_L3_016)
  Reason: supplies the foundational view of sets and real valued quantities in which heights, radicals, and logarithmic profiles are treated as well defined analytic summaries.

* Q019 (BH_MATH_DIOPH_DENSITY_L3_019)
  Reason: provides general tools for describing densities of Diophantine sets, reused here to talk about the density and distribution of abc exceptional triples.

* Q014 (BH_MATH_BOMB_LANG_L3_014)
  Reason: encodes large scale Diophantine geometry constraints that overlap with abc style tension between rational points, heights, and complexity.

### 2.2 Downstream problems

These nodes reuse Q005 components or assume its tension structure.

* Q015 (BH_MATH_RANK_BOUNDS_L3_015)
  Reason: reuses the HeightRadicalDescriptor and ABCQualityFunctional to express how bounds on heights and radicals influence rank bounds for elliptic curves.

* Q003 (BH_MATH_BSD_L3_003)
  Reason: uses abc based consistency_tension patterns when relating L function behavior to arithmetic invariants of elliptic curves.

* A future node for Szpiro type conjectures (for example Q0xx)
  Reason: directly recasts Szpiro conditions as a special case of the abc tension functional on elliptic curve invariants.

### 2.3 Parallel problems

Parallel nodes share similar tension types but no direct component dependence.

* Q001 (BH_MATH_NUM_L3_001)
  Reason: both Q001 and Q005 express strict compatibility conditions between hidden arithmetic structure and visible analytic summaries, and both use small tension bands versus persistent tension gaps as the organizing idea.

* Q002 (BH_MATH_GRH_L3_002)
  Reason: Q002 describes consistency_tension between families of L functions and arithmetic data, in parallel to the way Q005 relates additive equations, radicals, and heights.

### 2.4 Cross domain edges

Cross domain edges connect Q005 to nodes outside pure number theory that can reuse its components.

* Q059 (BH_CS_INFO_THERMODYN_L3_059)
  Reason: reuses the classical pattern “too many high quality exceptions cost global consistency” as an information theoretic tension between compression and constraint.

* Q123 (BH_AI_INTERP_L3_123)
  Reason: uses the idea of sparse high quality configurations under a simple relation as a model for interpreting neural representations that satisfy simple constraints yet rarely achieve extreme scores.

* Physical nodes such as Q036 that involve sparse extreme configurations
  Reason: draw on the ABCQualityFunctional and ABCCounterfactualPattern to describe when physical systems exhibit too many extreme states relative to simple conservation laws.

---

## 3. Tension Universe encoding (effective layer)

All content in this block is at the effective layer. We describe:

* the state space,
* observables and fields,
* invariants and tension functionals,
* the singular set and domain restrictions,
* an admissible class of encodings with explicit fairness constraints,
* an MVP-E2 harness choice that freezes otherwise free parameters into a small finite library.

We do not describe any deep generative rule or how states are constructed from raw data. We do not claim or use any proof or disproof of the canonical abc conjecture.

All mismatch terms `DeltaS_*` and tension values in this file are treated as **dimensionless or normalized quantities**, defined up to a fixed monotone rescaling specified in the TU Tension Scale Charter.

### 3.1 State space and triple library generation

We define a state space

```txt
M_abc
```

with this interpretation:

* Each state `m` in `M_abc` represents a finite resolution snapshot of abc relevant data.
* A state contains, at minimum:

  * a finite library of coprime integer triples `(a, b, c)` with `a + b = c` and `abc != 0`,
  * for each triple, a height summary `H(a, b, c)`,
  * for each triple, a radical summary `rad_abc(a, b, c)`,
  * per scale band, aggregate summaries described below,
  * metadata about the triple generation policy and coverage.

We do not specify any deep TU rule for how such states are computed or generated. For the purpose of this problem, we treat the triple library as produced by a **frozen, auditable search policy**:

```txt
SearchPolicy_abc
```

which must satisfy:

1. It is specified at the effective layer, for example

   * “enumerate all coprime triples `(a, b, c)` with `a + b = c`, `abc != 0` and `H(a,b,c) <= H_max`, in lexicographic order”,

   or another fully described deterministic procedure.

2. The policy and all parameters (such as `H_max`) are recorded in the metadata of `m`, including a hash of the code or configuration file used, so that an external observer can in principle rerun the search and rebuild the same library.

States that encode triple libraries not produced by a declared deterministic `SearchPolicy_abc` are considered out of the admissible class for Q005 experiments.

### 3.2 Height, radical, quality and scale bands

We fix the following effective definitions, using natural logarithms throughout.

**Height.**

For each triple we define the height as:

```txt
H(a, b, c) = max(|a|, |b|, |c|).
```

**Radical.**

We define the radical based on absolute values:

```txt
rad_abc(a, b, c) = rad(|a b c|),
```

where `rad(n)` is the product of the distinct prime divisors of `n`.

**Quality.**

For a single triple we define the quality

```txt
q(a, b, c) = log(H(a, b, c)) / log(rad_abc(a, b, c)),
```

whenever both logarithms are defined and positive. Triples for which `log(rad_abc(a,b,c))` is not positive are excluded from quality based summaries and are handled separately in the singular set definition.

**Scale bands.**

We fix a finite index set

```txt
Scale_abc = {k_0, k_1, ..., k_{K-1}}
```

with bands based on geometric growth in height. For some choice of `H_min > 1` and `r > 1` we set

```txt
B_k = { (a, b, c) :
        H(a, b, c) in [H_min r^k, H_min r^(k+1)) },
```

for `k = 0, 1, ..., K-1`.

The values `H_min`, `r`, `K` and the corresponding bands are part of the encoding choice. In the MVP-E2 harness (Section 6.3) they are frozen to concrete numerical values taken from a small finite template library.

### 3.3 Bandwise observables

For each state `m` in `M_abc` and each scale index `k` in `Scale_abc`, the triple library determined by `SearchPolicy_abc` induces a finite subset of triples in band `B_k`. From this subset we define the following effective observables.

1. Height observable

```txt
H(m; k) >= 1
```

A scalar summary of the heights of triples in band `B_k`, for example the average or a typical value of `H(a, b, c)`. The exact form is part of the encoding choice but must be fixed independently of the particular state once an encoding is selected.

2. Radical observable

```txt
R(m; k) >= 1
```

A scalar summary of the radicals `rad_abc(a, b, c)` in band `B_k`.

3. Quality observable (frozen definition)

At the level of individual triples we use `q(a, b, c)` as above. At the band level we **fix** the quality observable to be the upper quantile:

```txt
Q(m; k) = 95th percentile of q(a, b, c)
          over triples in B_k produced by SearchPolicy_abc,
```

whenever at least a minimal number of triples are present in the band. This is a hard choice: `Q` is not allowed to be mean or median in this specification. It is an upper tail summary focusing on high quality triples.

4. Exceptional density observable

We fix a positive threshold `eps > 0` and, for the MVP-E2 harness, we set

```txt
eps_k = eps  for all k in Scale_abc.
```

For a state `m`, we define

```txt
D_exc(m; k) in [0, 1]
```

as the fraction of triples in band `B_k` that satisfy

```txt
q(a, b, c) > 1 + eps_k.
```

The fraction is taken relative to the number of triples in `B_k` generated by the frozen `SearchPolicy_abc`. It is not defined relative to arbitrary user-selected samples.

### 3.4 Coverage metadata

To avoid hidden sampling bias, we attach a coverage observable to each band.

For each state `m` and band index `k` we define

```txt
Cov(m; k) in [0, 1]
```

as an estimate of the coverage of band `B_k` by `SearchPolicy_abc`. Typical cases:

* If `SearchPolicy_abc` provably enumerates all triples with `H(a,b,c) <= H_max` and the band lies below `H_max`, then `Cov(m; k) = 1`.
* If `SearchPolicy_abc` uses a time limited or heuristic search, `Cov(m; k)` reflects the known or declared lower bound on the fraction of band `B_k` that has been explored.

The encoding must specify a minimal coverage threshold `Cov_min in (0, 1]`. For any band with

```txt
Cov(m; k) < Cov_min
```

the observables `Q(m; k)` and `D_exc(m; k)` are not used in the global tension functional. That band is treated as out-of-domain for the purpose of tension aggregation.

The values of `Cov_min` for specific harnesses are part of the encoding choice and are declared ahead of any data dependent evaluation.

### 3.5 Mismatch fields

We now define two nonnegative mismatch observables per band, treated as dimensionless quantities that live on the TU tension scale.

1. Quality mismatch

```txt
DeltaS_quality(m; k) >= 0
```

This measures how much `Q(m; k)` deviates from an abc compatible reference profile at scale `k`. The reference profile

```txt
Q_ref(k; theta_Q)
```

is chosen from a **finite template library** of analytic curves indexed by a parameter `theta_Q` that belongs to a finite set. Once a template and parameter value are selected for a given encoding, they are fixed and recorded along with a version identifier.

2. Density mismatch

```txt
DeltaS_density(m; k) >= 0
```

This measures how much `D_exc(m; k)` deviates from an abc compatible upper bound at scale `k`. For each scale we choose an upper bound of the form

```txt
u_k(γ) = 1 / log(H_max(k))^γ
```

where `γ` is chosen from a small finite set (for example `{1, 2}`) and `H_max(k)` is a typical upper height for band `B_k`. The collection of `(γ, H_max(k))` values used in a given encoding is part of the encoding metadata.

For both mismatch fields we require:

```txt
DeltaS_quality(m; k) = 0 and DeltaS_density(m; k) = 0
```

when the observed summaries match the chosen abc compatible profiles for that encoding at scale `k`. Mismatch values grow as deviations increase, but their absolute scale is normalized according to the TU Tension Scale Charter.

Bands with `Cov(m; k) < Cov_min` are excluded from mismatch computation or are treated as contributing an “undefined” value, which feeds into the singular set in Section 3.9.

### 3.6 Combined abc tension functional

We define the abc tension functional as a finite weighted sum over the scale library, restricted to bands with sufficient coverage.

Let

```txt
Scale_abc_reg(m) =
  { k in Scale_abc : Cov(m; k) >= Cov_min }.
```

We then define

```txt
Tension_abc(m) =
  sum over k in Scale_abc_reg(m) of
    [ alpha_k * DeltaS_quality(m; k)
      + beta_k * DeltaS_density(m; k) ].
```

The coefficients satisfy:

```txt
alpha_k >= 0,  beta_k >= 0  for all k in Scale_abc_reg(m),
sum over k in Scale_abc_reg(m) of (alpha_k + beta_k) = 1.
```

The weights `(alpha_k, beta_k)` are part of the encoding and are fixed once and for all when an encoding in the admissible class is chosen. They are not allowed to depend on the particular state `m` or on the data in its triple library.

`Tension_abc(m)` is then a nonnegative scalar that summarizes, across scales with adequate coverage, how far the observed data encoded in `m` are from the abc compatible pattern.

### 3.7 Admissible encoding class and fairness constraints

We define an admissible class of encodings `E_abc` with the following properties.

1. Finite scale library fixed in advance

* The finite set `Scale_abc` and the corresponding height bands `B_k` are chosen using simple geometric growth rules as in Section 3.2.
* The parameters `H_min`, `r`, `K` are selected from a finite menu and are recorded in the encoding metadata.

2. Thresholds fixed in advance

* A single `eps > 0` is chosen from a finite menu (for example `{0.1, 0.01, 0.001}`) and then fixed for all bands as `eps_k = eps`.
* Once chosen, `eps` is recorded with the encoding and not tuned after inspecting any particular state.

3. Reference profiles and upper bounds from finite libraries

* The reference profile `Q_ref(k; theta_Q)` is chosen from a finite template library of curves, and the parameter `theta_Q` is chosen from a finite set. This pair is then fixed for the encoding.
* Upper bounds `u_k(γ)` are chosen from a finite family parameterized by `γ` in a small finite set, with `H_max(k)` defined from the band bounds. These choices are fixed once selected.

4. Weights normalized and fixed

* Weights `alpha_k`, `beta_k` satisfy the normalization rule and are fixed once an encoding is chosen.
* They may depend on `k` but not on any state `m` or on observed data for that encoding.

5. Search policy and coverage declared

* The triple library in any state used for Q005 experiments must be generated by a fully specified deterministic `SearchPolicy_abc`.
* The coverage observable `Cov(m; k)` must be defined in a way that can be audited from the search policy and its parameters.
* Bands with `Cov(m; k) < Cov_min` are excluded from tension aggregation.

6. No data dependent tuning

* No element of the encoding (scale bands, thresholds, reference profiles, upper bounds, weights, coverage thresholds) is allowed to depend on the actual list of high quality triples in a given state.
* All such choices are made at the encoding level, guided by general Diophantine considerations and public benchmark selections, but not by the unknown truth value of the abc conjecture.

7. No hidden answer fields

* No encoding in `E_abc` is allowed to hide the canonical answer to the abc conjecture as an uninterpreted field, label, or parameter.
* The tension functional must be constructed from observable summaries only, with no direct label indicating “abc is true” or “abc is false”.

These fairness constraints are subordinate to and consistent with the TU Effective Layer Charter and the TU Encoding and Fairness Charter. They ensure that `Tension_abc(m)` cannot be made artificially small by altering the encoding after seeing the data.

### 3.8 Tension tensor link

At the effective layer, the abc tension functional feeds into the TU tension tensor in the standard form:

```txt
T_ij(m) = S_i(m) * C_j(m) * Tension_abc(m) * lambda(m) * kappa_abc.
```

Here:

* `S_i(m)` encodes the strength of the ith source component of abc relevant structure in state `m`.
* `C_j(m)` encodes the sensitivity of the jth cognitive or downstream component to abc related inconsistencies.
* `lambda(m)` is the convergence or divergence factor from the TU core decisions.
* `kappa_abc` is a fixed coupling constant that sets the scale of abc related consistency tension within the TU tension scale.

We do not specify the indexing sets for `i` and `j`, only that for each `m` all relevant tensor components are finite and well defined.

### 3.9 Singular set and regular domain

Some observables may become undefined or unbounded, for example if:

* a band contains no triples,
* logarithms cannot be formed in a consistent way,
* quality or density summaries fail to exist or are numerically unstable,
* coverage falls below the minimal threshold.

We collect such states into a singular set:

```txt
S_sing_abc = {
  m in M_abc :
    there exists k in Scale_abc such that
      (H(m; k), R(m; k), Q(m; k), D_exc(m; k)) is undefined or not finite
      while Cov(m; k) >= Cov_min,
    or Cov(m; k) is undefined for some k
}.
```

We restrict abc tension analysis to the regular domain:

```txt
M_abc_reg = M_abc \ S_sing_abc.
```

Whenever an experiment or reasoning step would require evaluating `Tension_abc(m)` for `m` in `S_sing_abc`, this is treated as “out of domain” and not as evidence for or against the abc conjecture.

---

## 4. Tension principle for this problem

This block describes abc as a tension statement using the encoding above.

### 4.1 Core tension principle

At the effective layer, abc is expressed as a principle about how additive and multiplicative structures must cohere across scales.

We define two kinds of worlds:

* abc compatible worlds, where high quality triples are sufficiently rare at every scale, and
* abc violating worlds, where high quality triples appear too often or with too large quality across many scales.

In terms of the tension functional:

* In an abc compatible world there should exist regular states `m_true` in `M_abc_reg` such that

  ```txt
  Tension_abc(m_true) <= epsilon_abc
  ```

  for some small bound `epsilon_abc` that depends on the chosen encoding but does not grow without control as the scale library is refined in a way consistent with the charters.

* In a strongly abc violating world, any regular state that faithfully encodes the actual triple distribution would satisfy

  ```txt
  Tension_abc(m_false) >= delta_abc
  ```

  for some strictly positive `delta_abc` that cannot be reduced to zero while staying within the admissible class `E_abc` and respecting the observed data.

### 4.2 Scaling and refinement behavior

When we refine the scale library by adding more bands or splitting existing bands, the encoding is updated within `E_abc` in a way that preserves the fairness constraints. The core expectations are:

* In an abc compatible world, refinements that keep the same underlying distribution of triples should keep `Tension_abc(m_true)` within a controlled band defined by the TU Tension Scale Charter.
* In an abc violating world with infinitely many high quality triples, refinements should eventually expose bands where `DeltaS_quality(m_false; k)` and `DeltaS_density(m_false; k)` remain large, and therefore keep `Tension_abc(m_false)` bounded away from zero.

The abc conjecture is therefore framed as the assertion that our universe belongs to the class of worlds in which low scale stable tension configurations exist for some encoding in `E_abc`.

---

## 5. Counterfactual tension worlds

We now describe two counterfactual worlds, strictly at the effective layer:

* World T: abc is true.
* World F: abc is false in a strong sense that produces many high quality exceptions.

These worlds differ only in the observed patterns of the abc relevant observables on states in `M_abc_reg`.

### 5.1 World T (abc true, low consistency tension)

In World T:

1. Sparse high quality triples

* For each scale index `k` there are at most finitely many triples with `q(a, b, c) > 1 + eps_k`, and the fraction of such triples in band `B_k` is extremely small.
* The observable `D_exc(m_T; k)` stays below the abc compatible upper bounds `u_k(γ)` at all scales where coverage is adequate.

2. Stability of aggregated quality

* The aggregated quality `Q(m_T; k)` stays in a band compatible with abc, across all `k` with sufficient coverage.
* As more triples are included and the data are refined, the deviations encoded in `DeltaS_quality(m_T; k)` remain small.

3. Global tension band

* The combined tension functional satisfies

  ```txt
  Tension_abc(m_T) <= epsilon_abc
  ```

  for all regular states `m_T` that encode the actual triple distribution at the chosen resolution.
* Refinements that respect the encoding class `E_abc` do not push `Tension_abc(m_T)` out of a narrow low tension band.

### 5.2 World F (abc false, persistent high consistency tension)

In World F:

1. Abundant high quality triples

* There exist infinitely many triples with `q(a, b, c) > 1 + eps_k` across infinitely many scales.
* For some scales, the observable `D_exc(m_F; k)` is large and does not shrink toward zero as more data are incorporated.

2. Large deviations in aggregated quality

* The aggregated quality `Q(m_F; k)` significantly exceeds abc compatible values at infinitely many scales.
* The mismatch `DeltaS_quality(m_F; k)` remains large in those scales even as the encoding is refined, within the permitted template library.

3. Global tension gap

* For any regular state `m_F` that faithfully reflects the actual triple distribution within an encoding in `E_abc`, the combined tension satisfies

  ```txt
  Tension_abc(m_F) >= delta_abc
  ```

  for some strictly positive `delta_abc`.
* Attempts to refine the encoding within `E_abc` cannot reduce this lower bound without contradicting the observed patterns or violating fairness.

### 5.3 Interpretive note

These worlds are not constructions of internal TU fields from raw data. They are patterns of observables and tension values. The difference between World T and World F is:

* whether the universe admits regular states with low and stable `Tension_abc`, or
* whether any faithful encoding must carry irreducible consistency tension that cannot be tuned away within the admissible class.

---

## 6. Falsifiability and discriminating experiments

This block specifies experiments and protocols that test:

* the coherence of the abc tension encoding,
* the ability of `Tension_abc` to distinguish abc compatible models from abc violating models,
* the robustness of parameter choices inside `E_abc`.

These experiments cannot prove or disprove the abc conjecture, but they can falsify specific encodings at the effective layer.

### Experiment 1: Numerical abc tension profile from known triples

**Goal.**
Test whether a given encoding in `E_abc` produces stable and interpretable `Tension_abc` values when applied to existing computational data of abc triples.

**Setup.**

* Input data: published or constructed databases of coprime triples `(a, b, c)` with `a + b = c` up to a height bound `H_max`, generated by a documented `SearchPolicy_abc`.
* Choose an encoding in `E_abc`:

  * fix a finite scale library `Scale_abc` with bands based on height ranges up to `H_max`,
  * fix a single threshold `eps` and thus `eps_k = eps` for all `k`,
  * choose reference profiles `Q_ref(k; theta_Q)` from a finite template library and upper bounds `u_k(γ)` from a finite family,
  * fix weights `alpha_k`, `beta_k` that satisfy the normalization rule,
  * fix a coverage threshold `Cov_min`.

**Protocol.**

1. Build a state `m_data` in `M_abc` that encodes the triple library, band membership, coverage estimates, and the chosen encoding metadata.
2. Check whether `m_data` lies in `M_abc_reg`. If not, adjust the encoding or the data preprocessing to remove singular issues and rebuild.
3. For each scale index `k` compute, whenever `Cov(m_data; k) >= Cov_min`:

   * `H(m_data; k)`,
   * `R(m_data; k)`,
   * `Q(m_data; k)`,
   * `D_exc(m_data; k)`,
   * `DeltaS_quality(m_data; k)`,
   * `DeltaS_density(m_data; k)`.
4. Compute `Tension_abc(m_data)` by the finite sum formula over `Scale_abc_reg(m_data)`.
5. Repeat for slightly varied encodings in `E_abc` that keep thresholds and weights in a reasonable band but do not depend on the actual triple list.

**Metrics.**

* The full vector of `DeltaS_quality(m_data; k)` and `DeltaS_density(m_data; k)` across scales.
* The value of `Tension_abc(m_data)` for each encoding tested.
* Sensitivity of `Tension_abc(m_data)` to small changes in thresholds and weights within the constraints of `E_abc`.

**Falsification conditions.**

* If all fair encodings in `E_abc` with reasonable parameter ranges produce `Tension_abc(m_data)` far above any plausible `epsilon_abc` based on abc compatible expectations, then the current encoding approach is considered falsified at the effective layer. The correct response is to revise or retire that encoding, not to claim abc is false.
* If small allowed modifications inside `E_abc` cause arbitrarily large swings in `Tension_abc(m_data)` without clear theoretical justification, the encoding is considered unstable and rejected.

**Semantics implementation note.**
All observables are treated as functions that map discrete triple data into real valued summaries in a way that matches the hybrid description in the metadata. No change of representation beyond this is assumed.

**Boundary note.**
Falsifying a TU encoding is not the same as solving the canonical abc conjecture.

---

### Experiment 2: Synthetic abc compatible and abc violating model worlds

**Goal.**
Check whether the abc tension encoding can reliably distinguish between synthetic worlds that are designed to mimic an abc compatible distribution and synthetic worlds that deliberately violate abc type sparsity.

**Setup.**

* Construct two synthetic model classes of triple distributions:

  * Class T (abc compatible models): triple distributions where only very few high quality triples are allowed in each scale band.
  * Class F (abc violating models): triple distributions where high quality triples occur with a positive density across infinitely many scales.

* For each model instance, generate a finite triple library up to a chosen height bound using a documented model specific analog of `SearchPolicy_abc`, and construct a corresponding state in `M_abc`.

**Protocol.**

1. Choose a single encoding in `E_abc` that will be used for both Class T and Class F.
2. For each model state `m_T_model` in Class T:

   * compute the vector of mismatch fields and `Tension_abc(m_T_model)`.
3. For each model state `m_F_model` in Class F:

   * compute the same quantities.
4. Aggregate the results into two distributions of tension values, one for Class T and one for Class F.

**Metrics.**

* Mean and variance of `Tension_abc` for Class T and for Class F.
* The separation between the two distributions, for example the difference between their means relative to their spreads.
* The fraction of cases where Class T models have tension above a given threshold compared to the fraction for Class F models.

**Falsification conditions.**

* If the encoding assigns similar `Tension_abc` values to Class T and Class F across many model instances, failing to separate abc compatible from abc violating patterns, then that encoding in `E_abc` is considered ineffective and rejected.
* If Class F models consistently receive lower `Tension_abc` values than Class T models, the encoding is misaligned with the intended consistency_tension interpretation and must be revised.

**Semantics implementation note.**
The same representation and summaries used for actual integer triples are applied to synthetic models, keeping a consistent treatment of discrete and continuous aspects.

**Boundary note.**
Again, falsifying a TU encoding is not the same as solving the canonical abc conjecture.

---

### 6.3 MVP-E2 harness specification

For the **MVP-E2** level of Q005 within the TU program, we freeze one concrete encoding in `E_abc` as follows.

1. Height

```txt
H(a, b, c) = max(|a|, |b|, |c|).
```

2. Scale bands

* Choose `H_min` and `r` from a small finite menu, for example `H_min = 10`, `r = 10`.
* Define bands

  ```txt
  B_k = { (a, b, c) :
          H(a, b, c) in [H_min r^k, H_min r^(k+1)) }
  ```

  for `k = 0, 1, ..., K-1`, with `K` chosen so that the top band reaches the maximum height supported by the available data.

3. Quality aggregator

```txt
Q(m; k) = 95th percentile of q(a, b, c)
          over triples in B_k produced by SearchPolicy_abc.
```

4. eps and exceptional density

* Fix a single constant `eps = 0.01` and set `eps_k = eps` for all `k`.
* Define `D_exc(m; k)` as the fraction of triples in `B_k` with `q(a, b, c) > 1 + eps`, relative to the triple count under `SearchPolicy_abc`.

5. Upper bounds and reference profiles

* Choose `γ` from `{1, 2}` and define

  ```txt
  u_k(γ) = 1 / log(H_max(k))^γ
  ```

  with `H_max(k)` determined from the upper height of `B_k`.
* Choose `Q_ref(k; theta_Q)` from a finite template library of simple decreasing functions of `k` with parameters restricted to a small finite set.

6. Weights and coverage

* Choose weights `alpha_k`, `beta_k` that satisfy the normalization rule and assign slightly higher weight to larger `k` bands, under a documented scheme.
* Fix `Cov_min` to a value such as `0.9` when exhaustive search is guaranteed, or to a smaller documented value when partial search is used.

7. Search policy

* Fix `SearchPolicy_abc` to an explicit deterministic enumeration, for example:

  * “enumerate all coprime triples `(a, b, c)` with `a + b = c`, `abc != 0` and `H(a,b,c) <= H_max` in lexicographic order”,

  with `H_max` chosen so that data is manageable but nontrivial.
* Record a hash of the implementation and parameters together with the tension profiles.

This MVP-E2 harness is not unique, but it is a concrete, conservative configuration that external users can rerun and audit. It respects the TU charters and the fairness constraints set out for `E_abc`.

---

## 7. AI and WFGY engineering spec

This block describes how Q005 can be used as an engineering module in AI systems at the effective layer.

### 7.1 Training signals

We define training signals that rely on the abc tension structure without exposing any deep TU rules.

1. `signal_abc_quality_penalty`

   * Definition: a nonnegative signal proportional to a weighted sum of `DeltaS_quality(m; k)` over scales that are relevant in the current context.
   * Purpose: penalize internal states or reasoning traces where the model implicitly predicts too many high quality triples under contexts that assume abc compatible behavior.

2. `signal_abc_exception_sparsity`

   * Definition: a signal built from `D_exc(m; k)` and the abc compatible upper bounds `u_k(γ)`.
   * Purpose: encourage the model to treat high quality exceptions as sparse and special rather than typical when working under abc type assumptions.

3. `signal_abc_counterfactual_separation`

   * Definition: a signal that measures whether the model correctly keeps separate the consequences of assuming abc and assuming a strong abc violation in controlled prompts.
   * Purpose: reduce mixing between World T and World F narratives in multi step reasoning.

4. `signal_abc_tension_magnitude`

   * Definition: directly sets the signal equal to `Tension_abc(m)` for states that summarize the model’s internal beliefs about triples across scales.
   * Purpose: provide a scalar consistency indicator for use in auxiliary loss terms or interpretability tools.

### 7.2 Architectural patterns

We outline module patterns that reuse Q005 components.

1. `ABCTensionHead`

   * Role: given internal representations of Diophantine reasoning, estimate `Tension_abc(m)` as an auxiliary output.
   * Interface:

     * Input: internal embeddings that summarize facts about triples or abc style constraints.
     * Output: a scalar estimate of tension and, optionally, separate contributions from quality and density mismatch.

2. `RadicalHeightDescriptor`

   * Role: map textual or symbolic descriptions of integer relations into compact height and radical features.
   * Interface:

     * Input: a representation of an equation or family of triples.
     * Output: a low dimensional vector encoding approximate height, radical, and quality summaries suitable for tension evaluation.

3. `DiophantineConsistencyFilter`

   * Role: act as a soft filter on candidate statements in number theory discussions.
   * Interface:

     * Input: candidate statements about infinite families of triples or inequalities.
     * Output: a score indicating whether, under abc assumptions, the statement would imply too many high quality exceptions and therefore high consistency tension.

### 7.3 Evaluation harness

We propose an evaluation harness that can be used to test AI systems augmented with Q005 modules.

1. Task selection

   * Collect tasks where abc plays a known role, for example:

     * explaining abc and its consequences,
     * deciding whether simple Diophantine claims are plausible under abc,
     * comparing two statements where one is known to follow from abc and one is not.

2. Conditions

   * Baseline condition:

     * The model operates without specific abc tension modules.
   * TU condition:

     * The model uses ABCTensionHead and DiophantineConsistencyFilter as auxiliary modules, with training signals based on Q005 observables.

3. Metrics

   * Accuracy on questions that explicitly assume abc.
   * Rate at which the model incorrectly claims that abc implies statements that are not known to follow.
   * Internal consistency of answers when the prompt switches between assuming abc and assuming its failure.

4. Analysis

   * Compare baseline and TU configurations to see whether the presence of Q005 based modules improves consistency and interpretability without causing systematic new errors.

### 7.4 60 second reproduction protocol

A minimal procedure for external users to perceive the effect of Q005 encoding.

* Baseline setup

  * Prompt the AI:

    ```txt
    Explain the abc conjecture, give its standard statement,
    and list three important consequences.
    Do not mention any "tension" concepts.
    ```

  * Record the answer, including how well it connects radical, height, and the rarity of high quality triples.

* TU encoded setup

  * Prompt the AI:

    ```txt
    Explain the abc conjecture using the idea of consistency tension
    between the additive relation a + b = c, the radical of abc,
    and the height of c. Describe how rare high quality triples
    should be if abc holds, and how you would summarize this as
    a tension functional.
    ```

  * Record the answer and any auxiliary tension values if the system exposes them.

* Comparison metric

  * Rate both responses on:

    * clarity of the abc statement,
    * explicit link between radical and height,
    * explanation of why high quality exceptions should be rare,
    * internal coherence of the narrative.

* What to log

  * The prompts, responses, and any tension scores produced by ABCTensionHead.
  * This supports later inspection and comparison across models and parameter settings.

---

## 8. Cross problem transfer template

This block records the reusable components produced by Q005 and where they transfer.

### 8.1 Reusable components produced by this problem

1. ComponentName: `ABCQualityFunctional`

   * Type: functional

   * Minimal interface:

     * Inputs: a compact summary of triple distributions, including approximate height and radical statistics across several scale bands.
     * Output: a vector of mismatch values `DeltaS_quality(m; k)` and a combined quality contribution to `Tension_abc(m)`.

   * Preconditions:

     * The input summaries must correspond to coprime triples with `a + b = c` and well defined heights and radicals.
     * Coverage information is available so that bands with unreliable statistics can be excluded.

2. ComponentName: `HeightRadicalDescriptor`

   * Type: field

   * Minimal interface:

     * Inputs: a collection of integer equations or Diophantine patterns.
     * Output: feature vectors that encode approximate height and radical information suitable for use in functionals like `ABCQualityFunctional`.

   * Preconditions:

     * The input equations allow a meaningful notion of height and radical, even if presented in symbolic or textual form.

3. ComponentName: `ABCCounterfactualPattern`

   * Type: experiment_pattern

   * Minimal interface:

     * Inputs: a model class that generates or describes families of Diophantine objects.
     * Output: a pair of experiment templates corresponding to:

       * a low tension world where high quality exceptions are sparse,
       * a high tension world where they occur too often.

   * Preconditions:

     * The model class provides access to enough structure to define analogues of quality and exceptional density observables.

### 8.2 Direct reuse targets

1. Q014 (Bombieri–Lang type conjecture)

   * Reused component:

     * `ABCQualityFunctional` and `HeightRadicalDescriptor`.

   * Why it transfers:

     * Bombieri–Lang style conjectures also express the idea that rational points with certain properties are rare relative to natural complexity measures.

   * What changes:

     * Instead of triples and `rad(abc)`, the descriptor focuses on rational points on varieties and geometric invariants.

2. Q015 (uniform rank bounds for elliptic curves)

   * Reused component:

     * `HeightRadicalDescriptor` and `ABCCounterfactualPattern`.

   * Why it transfers:

     * Rank bounds can be linked to abc type inequalities, and the same pattern of “few very large height exceptions” versus “many exceptions” appears.

   * What changes:

     * The observables summarize heights and conductors of elliptic curves rather than triples, but the functional role is similar.

3. Q003 (Birch and Swinnerton–Dyer conjecture)

   * Reused component:

     * `ABCCounterfactualPattern`.

   * Why it transfers:

     * BSD connects analytic invariants to arithmetic invariants in a way that can be framed as consistency_tension; abc style patterns become part of the constraint backdrop.

   * What changes:

     * The pattern is applied to L function data and ranks instead of pure triple data.

4. Q059 (information and thermodynamic tension in computation)

   * Reused component:

     * `ABCQualityFunctional` as an abstract template for counting rare high quality configurations against an overall complexity budget.

   * Why it transfers:

     * The idea that too many extreme configurations violate resource bounds is common to both settings.

   * What changes:

     * The meaning of “quality” and “radical” is replaced by information content and resource usage metrics.

---

## 9. TU roadmap and verification levels

This block explains where Q005 currently sits in the TU program and what the next measurable steps are.

### 9.1 Current levels

* E_level: E1

  * The effective state space `M_abc`, observables, mismatch fields, abc tension functional, admissible encoding class, and singular set are clearly specified.
  * At least one concrete experiment template exists to test and potentially falsify specific encodings in `E_abc`.

* N_level: N2

  * The narrative of abc as a consistency_tension principle is explicit.
  * Counterfactual worlds and AI engineering modules are defined in a coherent way at the effective layer.

The MVP-E2 harness specified in Section 6.3 gives a concrete path to raise E_level once implemented and exercised.

### 9.2 Next measurable step toward E2

To advance Q005 from E1 to E2, one or more of the following steps should be carried out:

1. Implement a prototype tool that:

   * takes as input a finite database of abc triples generated by a documented `SearchPolicy_abc`,
   * constructs a regular state `m_data` in `M_abc_reg`,
   * computes `DeltaS_quality(m_data; k)`, `DeltaS_density(m_data; k)`, and `Tension_abc(m_data)`,
   * publishes the resulting tension profiles along with the chosen encoding parameters and hashes in `E_abc`.

2. Run synthetic experiments as in Experiment 2 and publish:

   * the construction of Class T and Class F model worlds,
   * the distribution of `Tension_abc` for each class,
   * a clear separation analysis.

Both steps stay at the effective layer, since they operate on observable summaries and explicit encodings without exposing deeper TU generative mechanisms or making any claim about the truth of abc.

### 9.3 Long term role in the TU program

In the longer term, Q005 is expected to become:

* a central reference node for consistency_tension in Diophantine settings,
* a reusable template for any problem that asserts the rarity of extreme configurations under simple relations,
* a bridge between pure number theoretic conjectures and more general ideas about how “too many miracles” would force persistent tension in any coherent model.

As other S level Diophantine problems are encoded, Q005 will serve as a calibration point for how tightly TU style tension functionals can express difficult conjectures without claiming proofs.

---

## 10. Elementary but precise explanation

This block gives an accessible explanation that stays faithful to the effective layer description.

The abc conjecture starts from a very simple equation:

```txt
a + b = c,
```

where `a`, `b`, and `c` are whole numbers that share no common prime factor and none of them is zero.

There are two very different ways to look at the triple `(a, b, c)`.

1. Additive side

   * The equation `a + b = c` is as simple as possible.

2. Multiplicative side

   * Look at the primes that divide `a`, `b`, and `c`.
   * Multiply each distinct prime only once.
   * This gives the number `rad(abc)`.

Now define:

* the height

  ```txt
  H(a, b, c) = max(|a|, |b|, |c|),
  ```

  which measures how large the triple is,

* the quality

  ```txt
  q(a, b, c) = log(H(a, b, c)) / log(rad(abc)),
  ```

  which compares the size of `c` to the product of the distinct primes in `abc`, after taking logarithms to compress both.

If `q(a, b, c)` is a little larger than `1`, it means that `c` is significantly larger than the product of the distinct primes that appear in `abc`, even after using logarithms to compress both.

The abc conjecture says, roughly:

> Triples with very large quality should be extremely rare.

In the Tension Universe view, we do not try to prove this. Instead we ask how to measure the “tension” between:

* the simple equation `a + b = c`,
* the radical `rad(abc)`,
* the size of `c`.

We do this by:

1. Grouping triples into scale bands according to their height.
2. Summarizing, in each band:

   * how large the typical quality is, using a fixed rule (for example, the 95th percentile of quality),
   * what fraction of triples have quality above a fixed threshold.
3. Comparing these summaries with what abc would lead us to expect if high quality triples were truly rare.

This gives two mismatch quantities per band:

* how far the high-end quality is from an abc compatible profile,
* how far the fraction of high quality triples is from an abc compatible upper bound.

We combine these mismatches (with fixed weights) across bands into a single nonnegative number called `Tension_abc`. In an abc compatible world, for some reasonable way of choosing bands and weights that is fixed in advance, this tension number should stay small and reasonably stable when we look at more and more data. In a world where abc fails in a strong way, the tension should eventually become large and stay large, no matter how we choose bands and weights, as long as we respect fairness.

This approach does not answer the original yes or no question for abc. Instead it does three things:

1. It turns abc into a statement about low tension versus high tension patterns in observable summaries of triples.
2. It defines experiments that can reject bad encodings of this idea and sharpen good ones, without pretending to prove abc.
3. It produces reusable tools for other problems where a simple relation and a complexity measure must fit together without allowing too many extreme exceptions.

Q005 is therefore the Diophantine consistency template inside the Tension Universe, capturing the idea that “too many miracles” in number theory would show up as persistent, measurable tension in any coherent effective layer encoding of the arithmetic world.

---

## Tension Universe effective-layer footer

This page is part of the **WFGY / Tension Universe** S-problem collection. It follows the shared TU charters and the Q001–Q125 S-problem constitution.

### Scope of claims

* The goal of this document is to specify an **effective-layer encoding** of the abc conjecture as a consistency_tension problem.
* It does not claim to prove or disprove the canonical statement in Section 1.
* It does not introduce any new theorem beyond what is already established in the cited literature.
* It should not be cited as evidence that the abc conjecture has been solved.

### Effective-layer boundary

* All objects used here (state spaces `M`, observables, invariants, tension scores, counterfactual “worlds”) live at the effective layer.
* No step in this file gives a constructive mapping from raw experimental or simulation data into internal TU fields.
* No step exposes any TU deep generative rule, first-principle axiom system, or internal semantic wiring.
* Falsifying or revising a TU encoding is not the same as deciding the truth value of the abc conjecture.

### Encoding and fairness

This page should be read together with the following charters:

* [TU Effective Layer Charter](../Charters/TU_EFFECTIVE_LAYER_CHARTER.md)
* [TU Encoding and Fairness Charter](../Charters/TU_ENCODING_AND_FAIRNESS_CHARTER.md)
* [TU Tension Scale Charter](../Charters/TU_TENSION_SCALE_CHARTER.md)
* [TU Global Guardrails](../Charters/TU_GLOBAL_GUARDRAILS.md)

*Admissible encoding classes, reference profiles, upper bounds and weight families used in this page are constrained by the shared Tension Universe charters above.*

* For every encoding class referenced here:

  * its definition, parameter ranges, template families, and reference profiles are fixed at the charter or encoding level before any problem-specific tuning;
  * these choices may depend on general mathematical considerations and on public benchmark selections, but not on the unknown truth value of the abc conjecture;
  * no encoding is allowed to hide the canonical answer as an uninterpreted field, label or parameter.

* Tension encodings and reference families must respect all known theorems and hard constraints in the relevant mathematical domains. If a conflict is found, the encoding is revised or retired, not used to reinterpret those results.

### Tension scale and thresholds

* All mismatch terms `DeltaS_*` and tension functionals in this file are treated as **dimensionless or normalized quantities**, defined up to a fixed monotone rescaling specified in the TU Tension Scale Charter.
* Thresholds such as `epsilon_abc`, `delta_abc`, `eps`, and experiment cutoffs are always interpreted relative to that fixed scale.
* Changing the tension scale requires an explicit update of the TU Tension Scale Charter, not an edit of individual problem files.

### Falsifiability and experiments

* Experiments described in this document are **tests of TU encodings**, not tests of the underlying canonical problem itself.
* The rule “falsifying a TU encoding is not the same as solving the canonical statement” applies globally, even where it is not restated in the main text.
* When required observables cannot be reliably estimated in practice, the outcome of the corresponding experiment is recorded as “inconclusive”, not as confirmation of abc or its negation.

### Interaction with established results

* All encodings and counterfactual worlds described here are required to respect known theorems and hard constraints in Diophantine number theory and related fields.
* If a later analysis finds a concrete conflict with established results, the correct procedure is to update or retire the encoding under the TU charters, not to reinterpret those results.

### Program note

* This page is an experimental specification within the ongoing **WFGY / Tension Universe** research program.
* All structures and parameter choices are provisional and may be revised in future versions, subject to the constraints above.
* Any future revision must preserve the separation between effective-layer encodings and deeper TU mechanisms, and must continue to avoid embedding the truth value of the abc conjecture directly into the encoding.
