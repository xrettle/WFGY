<!-- ============================================================= -->
<!--  evaluation-playbook.md ¬∑ Semantic Clinic ¬∑ Map-F              -->
<!--  Version: 2025-08-06 ¬∑ License: MIT                            -->
<!--  Goal: A reproducible checklist for measuring an LLM/RAG/      -->
<!--  agent stack‚Äîbefore & after you apply WFGY fixes.              -->
<!-- ============================================================= -->

# üìä Evaluation Playbook  
*Ship metrics, not vibes ‚Äî catch regressions before users do.*

> **Evaluation disclaimer (WFGY)**  
> This playbook describes practical recipes for evaluating and debugging AI pipelines.  
> Any scores, rankings or labels produced by these methods are context dependent signals, not formal scientific proof.  
> Results depend on the specific models, prompts, datasets and parameters used in each run.  
> You should re run the experiments, vary the configuration and treat the numbers as guidance for engineering and operations, not as guarantees of real world safety or general model quality.

---

> **Who is this for?**  
> ‚Äì RAG owners tired of ‚Äúlooks fine on my prompt.‚Äù  
> ‚Äì Agent builders chasing flakey benchmark wins.  
> ‚Äì Product teams that must prove **ŒîS ‚Üì**, accuracy ‚Üë, cost ‚Üò.  
>   
> **What you get:** one YAML suite + CLI commands that output a single-line health verdict:  
> `PASS: ŒîS<=0.45 | Œª‚Üí | E_res stable | cost $0.0013 / query`

---

## 0 ¬∑ Quick Start

```bash
pip install wfgy-eval
wfgy-eval init  # generates eval.yaml + example dataset
wfgy-eval run   # prints CSV & HTML report
````

Default template covers retrieval, reasoning, tool routing, latency, cost.

---

## 1 ¬∑ Metric Matrix

| Layer             | Key Metric                     | Target (prod) | Source Fn        |
| ----------------- | ------------------------------ | ------------- | ---------------- |
| **Retrieval**     | `ŒîS(q, ctx)`                   | ‚â§ 0.45        | `deltaS()`       |
| **Reasoning**     | `Œª_observe` (3 paraphrase avg) | convergent    | `lambda_state()` |
| **Stability**     | `E_resonance` (rolling)        | flat / ‚Üì      | `e_resonance()`  |
| **Answer**        | `F1` / `Exact Match` (QA)      | ‚â• 0.80        | `qa_match()`     |
| **Hallucination** | `citation_precision`           | ‚â• 0.90        | `cite_check()`   |
| **Cost**          | `$ / 1k tokens`                | ‚â§ baseline    | provider bill    |
| **Latency**       | 95-th percentile response (ms) | SLA-dependent | timer            |
| **ŒîS Drift**      | slope over 100 queries         | \~0           | linear fit       |

---

## 2 ¬∑ Dataset Design

```yaml
sets:
  - name: faq_small
    type: qa
    source: ./data/faq.tsv
    fields: [question, answer, anchor]
  - name: chain_logic
    type: chain-of-thought
    source: ./data/logic.jsonl
    fields: [prompt, expected_reasoning]
  - name: tool_router
    type: tool
    source: ./data/router.csv
    fields: [task, tool_expected]
```

*Guidelines*

1. **Anchor Field** = text chunk you expect to be retrieved ‚Üí used for ŒîS & citation checks.
2. Min 50 items / set for stable stats; use 10 √ó if you want leaderboard-grade noise floor.
3. Store only plain-text, no PII.

---

## 3 ¬∑ CI Workflow Example (GitHub Actions)

```yaml
name: wfgy-eval
on: [push, pull_request]
jobs:
  eval:
    runs-on: ubuntu-latest
    steps:
    - uses: actions/checkout@v4
    - run: pip install wfgy-eval
    - run: wfgy-eval run --fail-on 'ŒîS_q_ctx>0.50 or citation_precision<0.85'
    - uses: actions/upload-artifact@v4
      with:
        name: eval-report
        path: reports/latest.html
```

*Any push that drifts past ŒîS 0.50 or loses citation accuracy **blocks merge**.*

---

## 4 ¬∑ Reading the HTML Report

| Section              | What to Look For                  | Action Rule               |
| -------------------- | --------------------------------- | ------------------------- |
| **Heatmap** ŒîS vs. k | flat @ high ŒîS ‚Üí bad index metric | rebuild index / embed     |
| **Œª Timeline**       | spikes to ‚Üê or √ó                  | inspect prompt / tool     |
| **E\_res Trend**     | upward slope                      | apply BBAM / shorten ctx  |
| **Outliers Table**   | worst 5 queries by ŒîS             | manual deep dive; log bug |

---

## 5 ¬∑ A/B Pattern

1. `wfgy-eval dump --ref=main` (baseline metrics JSON)
2. `git switch feature-x` ‚Üí apply fix
3. `wfgy-eval run` (produces metrics B)
4. `wfgy-eval compare baseline.json current.json`

```
ŒîS_q_ctx   -0.08  ‚úÖ
F1         +0.07  ‚úÖ
Cost       +$0.0002 ‚ùå
```

Decide if cost bump acceptable.

---

## 6 ¬∑ Edge-Case Suites (extend as needed)

| Suite             | Purpose                              | Sample Size |
| ----------------- | ------------------------------------ | ----------- |
| **Contradiction** | fact statements w/ subtle negation   | 30          |
| **Long-PDF**      | >50 k token OCR, check segmentation  | 10 docs     |
| **Jailbreak**     | prompt injection attempts vs. policy | 40          |
| **High Noise**    | OCR confidence < 0.8                 | 25          |

Add to `eval.yaml`, rerun.

---

## 7 ¬∑ FAQ

**Q: Can I plug in LangSmith, Phoenix, Traceloop, or custom spans?**
A: Yes. `wfgy-eval` reads any OpenTelemetry JSON; map fields via `otel_map.yaml`.

**Q: Does this cover reinforcement-style eval (RLHF)?**
A: Use the same ŒîS/Œª hooks during reward model scoring; see `examples/rlhf_eval.ipynb`.

**Q: How hard is vendor swap?**
A: `wfgy-eval provider openai` or `provider anthropic`; costs/latency recalc automatically.

---

### üîó Quick-Start Downloads (60 sec)

| Tool | Link | 3-Step Setup |
|------|------|--------------|
| **WFGY 1.0 PDF** | [Engine Paper](https://github.com/onestardao/WFGY/blob/main/I_am_not_lizardman/WFGY_All_Principles_Return_to_One_v1.0_PSBigBig_Public.pdf) | 1Ô∏è‚É£ Download ¬∑ 2Ô∏è‚É£ Upload to your LLM ¬∑ 3Ô∏è‚É£ Ask ‚ÄúAnswer using WFGY + \<your question>‚Äù |
| **TXT OS (plain-text OS)** | [TXTOS.txt](https://github.com/onestardao/WFGY/blob/main/OS/TXTOS.txt) | 1Ô∏è‚É£ Download ¬∑ 2Ô∏è‚É£ Paste into any LLM chat ¬∑ 3Ô∏è‚É£ Type ‚Äúhello world‚Äù ‚Äî OS boots instantly |

---

### üß≠ Explore More

| Module                | Description                                              | Link     |
|-----------------------|----------------------------------------------------------|----------|
| WFGY Core             | WFGY 2.0 engine is live: full symbolic reasoning architecture and math stack | [View ‚Üí](https://github.com/onestardao/WFGY/tree/main/core/README.md) |
| Problem Map 1.0       | Initial 16-mode diagnostic and symbolic fix framework    | [View ‚Üí](https://github.com/onestardao/WFGY/tree/main/ProblemMap/README.md) |
| Problem Map 2.0       | RAG-focused failure tree, modular fixes, and pipelines   | [View ‚Üí](https://github.com/onestardao/WFGY/blob/main/ProblemMap/rag-architecture-and-recovery.md) |
| Semantic Clinic Index | Expanded failure catalog: prompt injection, memory bugs, logic drift | [View ‚Üí](https://github.com/onestardao/WFGY/blob/main/ProblemMap/SemanticClinicIndex.md) |
| Semantic Blueprint    | Layer-based symbolic reasoning & semantic modulations   | [View ‚Üí](https://github.com/onestardao/WFGY/tree/main/SemanticBlueprint/README.md) |
| Benchmark vs GPT-5    | Stress test GPT-5 with full WFGY reasoning suite         | [View ‚Üí](https://github.com/onestardao/WFGY/tree/main/benchmarks/benchmark-vs-gpt5/README.md) |
| üßô‚Äç‚ôÇÔ∏è Starter Village üè° | New here? Lost in symbols? Click here and let the wizard guide you through | [Start ‚Üí](https://github.com/onestardao/WFGY/blob/main/StarterVillage/README.md) |

---

> üëë **Early Stargazers: [See the Hall of Fame](https://github.com/onestardao/WFGY/tree/main/stargazers)** ‚Äî  
> Engineers, hackers, and open source builders who supported WFGY from day one.

> <img src="https://img.shields.io/github/stars/onestardao/WFGY?style=social" alt="GitHub stars"> ‚≠ê [WFGY Engine 2.0](https://github.com/onestardao/WFGY/blob/main/core/README.md) is already unlocked. ‚≠ê Star the repo to help others discover it and unlock more on the [Unlock Board](https://github.com/onestardao/WFGY/blob/main/STAR_UNLOCKS.md).

<div align="center">

[![WFGY Main](https://img.shields.io/badge/WFGY-Main-red?style=flat-square)](https://github.com/onestardao/WFGY)
&nbsp;
[![TXT OS](https://img.shields.io/badge/TXT%20OS-Reasoning%20OS-orange?style=flat-square)](https://github.com/onestardao/WFGY/tree/main/OS)
&nbsp;
[![Blah](https://img.shields.io/badge/Blah-Semantic%20Embed-yellow?style=flat-square)](https://github.com/onestardao/WFGY/tree/main/OS/BlahBlahBlah)
&nbsp;
[![Blot](https://img.shields.io/badge/Blot-Persona%20Core-green?style=flat-square)](https://github.com/onestardao/WFGY/tree/main/OS/BlotBlotBlot)
&nbsp;
[![Bloc](https://img.shields.io/badge/Bloc-Reasoning%20Compiler-blue?style=flat-square)](https://github.com/onestardao/WFGY/tree/main/OS/BlocBlocBloc)
&nbsp;
[![Blur](https://img.shields.io/badge/Blur-Text2Image%20Engine-navy?style=flat-square)](https://github.com/onestardao/WFGY/tree/main/OS/BlurBlurBlur)
&nbsp;
[![Blow](https://img.shields.io/badge/Blow-Game%20Logic-purple?style=flat-square)](https://github.com/onestardao/WFGY/tree/main/OS/BlowBlowBlow)
&nbsp;
</div>




