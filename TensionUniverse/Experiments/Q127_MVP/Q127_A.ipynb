{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "JWvFdZayCKi9"
      },
      "outputs": [],
      "source": [
        "# WFGY 3.0 Singularity demo: Q127 Synthetic Worlds Entropy Gauge\n",
        "# One-cell Colab MVP: three tiny Gaussian worlds, one small MLP per world,\n",
        "# entropy-aware tension observable T_entropy, and a cross-world dashboard.\n",
        "#\n",
        "# If you do not have PyTorch / pandas / matplotlib in this runtime, uncomment:\n",
        "# !pip install --quiet torch pandas matplotlib\n",
        "\n",
        "import math\n",
        "import random\n",
        "from typing import Dict, List\n",
        "\n",
        "import numpy as np\n",
        "import pandas as pd\n",
        "import matplotlib.pyplot as plt\n",
        "import torch\n",
        "from torch import nn\n",
        "from torch.utils.data import DataLoader, TensorDataset\n",
        "\n",
        "\n",
        "# -------------------------------------------------------------------\n",
        "# 0. Header text: what this demo is and how to use it\n",
        "# -------------------------------------------------------------------\n",
        "\n",
        "header_text = \"\"\"\n",
        "WFGY 3.0 Singularity demo: Q127 Synthetic Worlds Entropy Gauge\n",
        "==============================================================\n",
        "\n",
        "This notebook is a small, fully inspectable MVP for TU Q127.\n",
        "It lives entirely at the effective layer. No weights are modified\n",
        "outside this script and there is no fine-tuning loop over real data.\n",
        "\n",
        "What Q127-A is asking\n",
        "---------------------\n",
        "\n",
        "We build three tiny synthetic worlds W1, W2 and W3:\n",
        "\n",
        "  - W1: balanced clean labels (class prior 0.5 / 0.5, no label noise).\n",
        "  - W2: imbalanced clean labels (class prior 0.1 / 0.9, no label noise).\n",
        "  - W3: balanced but noisy labels (class prior 0.5 / 0.5, labels flipped\n",
        "        with probability 0.2).\n",
        "\n",
        "Each world is a simple 2D Gaussian mixture binary classification problem.\n",
        "\n",
        "For each world W_i we train a small MLP classifier on samples from that\n",
        "world only. Then we evaluate every trained classifier on all three worlds.\n",
        "\n",
        "For every train->test pair we compute:\n",
        "\n",
        "  - accuracy on the test world,\n",
        "  - average cross entropy in bits,\n",
        "  - KL divergence in bits between the model label distribution and the\n",
        "    empirical label distribution of the test world,\n",
        "  - the difference in label entropies |H_label(train) - H_label(test)|,\n",
        "  - a scalar tension observable T_entropy in [0, 1] built from these pieces.\n",
        "\n",
        "We also define a very simple \"effective correctness\" flag:\n",
        "\n",
        "  - a pair is marked correct if accuracy >= 0.8 and KL <= 0.05.\n",
        "\n",
        "This is not a benchmark. The exact numbers move with the seed,\n",
        "the model size, and the sample counts. What matters is the pattern:\n",
        "\n",
        "  - T_entropy is small when a classifier is evaluated on the world it was\n",
        "    trained on, or on a very similar world,\n",
        "  - T_entropy grows when the classifier carries the \"wrong\" class balance\n",
        "    or noise structure in its beliefs about the test world.\n",
        "\n",
        "How to use this notebook\n",
        "------------------------\n",
        "\n",
        "1. Run this cell once, top to bottom.\n",
        "\n",
        "2. The script will:\n",
        "\n",
        "   - define the three worlds,\n",
        "   - print basic world statistics (class priors and entropies),\n",
        "   - train one small MLP per world,\n",
        "   - evaluate all train->test pairs and assemble a DataFrame,\n",
        "   - print a compact table of accuracy, CE, KL, deltaH and T_entropy,\n",
        "   - draw a 3x3 heatmap of T_entropy(train_world -> test_world).\n",
        "\n",
        "3. You can treat T_entropy as a crude \"world detector\":\n",
        "\n",
        "   - low tension on train==test pairs,\n",
        "   - higher tension when train and test worlds differ in class balance\n",
        "     or label noise.\n",
        "\n",
        "4. If you want to extend the experiment, you can change:\n",
        "\n",
        "   - the world definitions (means, priors, noise),\n",
        "   - the model (deeper MLP, different optimizer),\n",
        "   - the weights inside the T_entropy functional.\n",
        "\n",
        "Formal disclaimer\n",
        "-----------------\n",
        "\n",
        "This notebook does not claim to solve TU Q127 as a mathematical object\n",
        "or as a full benchmark. It only provides one small effective-layer\n",
        "experiment that can be inspected and re-run line by line.\n",
        "\"\"\"\n",
        "\n",
        "print(header_text)\n",
        "\n",
        "\n",
        "# -------------------------------------------------------------------\n",
        "# 1. Reproducibility setup\n",
        "# -------------------------------------------------------------------\n",
        "\n",
        "SEED = 127\n",
        "random.seed(SEED)\n",
        "np.random.seed(SEED)\n",
        "torch.manual_seed(SEED)\n",
        "\n",
        "\n",
        "# -------------------------------------------------------------------\n",
        "# 2. Synthetic world generators\n",
        "# -------------------------------------------------------------------\n",
        "\n",
        "def binary_entropy(p: float) -> float:\n",
        "    \"\"\"Shannon entropy of a Bernoulli(p) in bits.\"\"\"\n",
        "    if p <= 0.0 or p >= 1.0:\n",
        "        return 0.0\n",
        "    return -(p * math.log(p, 2.0) + (1.0 - p) * math.log(1.0 - p, 2.0))\n",
        "\n",
        "\n",
        "def generate_world(name: str,\n",
        "                   prior1: float,\n",
        "                   noise: float,\n",
        "                   n_samples: int = 600) -> Dict[str, object]:\n",
        "    \"\"\"\n",
        "    Generate one synthetic binary world in R^2.\n",
        "\n",
        "    Features:\n",
        "      - Class 0: Gaussian around [-1, 0]\n",
        "      - Class 1: Gaussian around [+1, 0]\n",
        "\n",
        "    Labels:\n",
        "      - y_clean ~ Bernoulli(prior1)\n",
        "      - y = y_clean, then flipped with probability `noise`.\n",
        "    \"\"\"\n",
        "    # Clean labels from the class prior\n",
        "    u = np.random.rand(n_samples)\n",
        "    y_clean = (u < prior1).astype(np.int64)\n",
        "\n",
        "    # Gaussian blobs for features\n",
        "    x = np.zeros((n_samples, 2), dtype=np.float32)\n",
        "    mean0 = np.array([-1.0, 0.0])\n",
        "    mean1 = np.array([+1.0, 0.0])\n",
        "    cov = 0.5 * np.eye(2)\n",
        "\n",
        "    idx0 = np.where(y_clean == 0)[0]\n",
        "    idx1 = np.where(y_clean == 1)[0]\n",
        "    x[idx0, :] = np.random.multivariate_normal(mean0, cov, size=len(idx0))\n",
        "    x[idx1, :] = np.random.multivariate_normal(mean1, cov, size=len(idx1))\n",
        "\n",
        "    # Apply label noise\n",
        "    y = y_clean.copy()\n",
        "    if noise > 0.0:\n",
        "        flips = np.random.rand(n_samples) < noise\n",
        "        y[flips] = 1 - y[flips]\n",
        "\n",
        "    # Effective label statistics after noise\n",
        "    p1_emp = float(y.mean())\n",
        "    H_label = binary_entropy(p1_emp)\n",
        "    H_noise = binary_entropy(noise) if 0.0 < noise < 1.0 else 0.0\n",
        "\n",
        "    return {\n",
        "        \"name\": name,\n",
        "        \"prior1\": float(prior1),\n",
        "        \"noise\": float(noise),\n",
        "        \"x\": x,\n",
        "        \"y\": y,\n",
        "        \"p1_empirical\": p1_emp,\n",
        "        \"H_label\": H_label,\n",
        "        \"H_noise\": H_noise,\n",
        "    }\n",
        "\n",
        "\n",
        "def make_worlds(n_samples: int = 600) -> List[Dict[str, object]]:\n",
        "    \"\"\"Define the three Q127-A worlds.\"\"\"\n",
        "    worlds = [\n",
        "        generate_world(\"W1_balanced_clean\", prior1=0.5, noise=0.0, n_samples=n_samples),\n",
        "        generate_world(\"W2_imbalanced_clean\", prior1=0.9, noise=0.0, n_samples=n_samples),\n",
        "        generate_world(\"W3_balanced_noisy\", prior1=0.5, noise=0.2, n_samples=n_samples),\n",
        "    ]\n",
        "    return worlds\n",
        "\n",
        "\n",
        "worlds = make_worlds(n_samples=600)\n",
        "\n",
        "print(\"\\nWorld definitions (empirical after noise):\")\n",
        "for w in worlds:\n",
        "    print(\n",
        "        f\"  {w['name']}: \"\n",
        "        f\"prior1={w['prior1']:.2f}, noise={w['noise']:.2f}, \"\n",
        "        f\"p1_emp={w['p1_empirical']:.3f}, \"\n",
        "        f\"H_label={w['H_label']:.3f} bits, \"\n",
        "        f\"H_noise={w['H_noise']:.3f} bits\"\n",
        "    )\n",
        "\n",
        "\n",
        "# -------------------------------------------------------------------\n",
        "# 3. Tiny MLP classifier per world\n",
        "# -------------------------------------------------------------------\n",
        "\n",
        "class TinyMLP(nn.Module):\n",
        "    \"\"\"Small 2-layer MLP for 2D binary classification.\"\"\"\n",
        "\n",
        "    def __init__(self) -> None:\n",
        "        super().__init__()\n",
        "        self.net = nn.Sequential(\n",
        "            nn.Linear(2, 16),\n",
        "            nn.ReLU(),\n",
        "            nn.Linear(16, 16),\n",
        "            nn.ReLU(),\n",
        "            nn.Linear(16, 1),\n",
        "            nn.Sigmoid(),\n",
        "        )\n",
        "\n",
        "    def forward(self, x: torch.Tensor) -> torch.Tensor:\n",
        "        return self.net(x)\n",
        "\n",
        "\n",
        "def train_classifier(world: Dict[str, object],\n",
        "                     epochs: int = 300,\n",
        "                     batch_size: int = 64,\n",
        "                     lr: float = 1e-2) -> TinyMLP:\n",
        "    \"\"\"Train a TinyMLP on samples from one world.\"\"\"\n",
        "    x_np = world[\"x\"]\n",
        "    y_np = world[\"y\"]\n",
        "\n",
        "    x = torch.from_numpy(x_np).float()\n",
        "    y = torch.from_numpy(y_np).float().unsqueeze(1)\n",
        "\n",
        "    ds = TensorDataset(x, y)\n",
        "    dl = DataLoader(ds, batch_size=batch_size, shuffle=True)\n",
        "\n",
        "    model = TinyMLP()\n",
        "    opt = torch.optim.Adam(model.parameters(), lr=lr)\n",
        "    loss_fn = nn.BCELoss()\n",
        "\n",
        "    for epoch in range(epochs):\n",
        "        model.train()\n",
        "        for xb, yb in dl:\n",
        "            opt.zero_grad()\n",
        "            preds = model(xb)\n",
        "            loss = loss_fn(preds, yb)\n",
        "            loss.backward()\n",
        "            opt.step()\n",
        "\n",
        "    return model\n",
        "\n",
        "\n",
        "print(\"\\nTraining one TinyMLP per world...\")\n",
        "models: Dict[str, TinyMLP] = {}\n",
        "for w in worlds:\n",
        "    print(f\"  Training on {w['name']} ...\")\n",
        "    m = train_classifier(w, epochs=300)\n",
        "    models[w[\"name\"]] = m\n",
        "\n",
        "\n",
        "# -------------------------------------------------------------------\n",
        "# 4. Effective-layer metrics and T_entropy\n",
        "# -------------------------------------------------------------------\n",
        "\n",
        "def evaluate_on_world(model: TinyMLP,\n",
        "                      train_world: Dict[str, object],\n",
        "                      test_world: Dict[str, object]) -> Dict[str, float]:\n",
        "    \"\"\"\n",
        "    Evaluate a trained classifier on a test world and compute:\n",
        "\n",
        "      - accuracy\n",
        "      - cross entropy in bits\n",
        "      - KL divergence in bits between model and world label distributions\n",
        "      - |H_label(train) - H_label(test)|\n",
        "      - scalar tension T_entropy in [0, 1]\n",
        "      - effective-layer correctness flag\n",
        "    \"\"\"\n",
        "    model.eval()\n",
        "    x_np = test_world[\"x\"]\n",
        "    y_np = test_world[\"y\"]\n",
        "\n",
        "    x = torch.from_numpy(x_np).float()\n",
        "    y_true = torch.from_numpy(y_np).float().unsqueeze(1)\n",
        "\n",
        "    with torch.no_grad():\n",
        "        probs = model(x).numpy().reshape(-1)\n",
        "\n",
        "    # Classification accuracy\n",
        "    preds = (probs >= 0.5).astype(np.int64)\n",
        "    acc = float((preds == y_np).mean())\n",
        "\n",
        "    # Cross entropy in bits\n",
        "    eps = 1e-8\n",
        "    ce_nat = -(\n",
        "        y_np * np.log(probs + eps) +\n",
        "        (1.0 - y_np) * np.log(1.0 - probs + eps)\n",
        "    ).mean()\n",
        "    ce_bits = float(ce_nat / math.log(2.0))\n",
        "\n",
        "    # Label distributions\n",
        "    p_hat1 = float(probs.mean())\n",
        "    p_hat = np.array([1.0 - p_hat1, p_hat1])\n",
        "\n",
        "    p_true1 = float(y_np.mean())\n",
        "    p_true = np.array([1.0 - p_true1, p_true1])\n",
        "\n",
        "    # KL(p_true || p_hat) in bits\n",
        "    kl = 0.0\n",
        "    for pt, ph in zip(p_true, p_hat):\n",
        "        if pt > 0.0 and ph > 0.0:\n",
        "            kl += pt * math.log(pt / ph, 2.0)\n",
        "    kl_bits = float(kl)\n",
        "\n",
        "    # Entropy gap |H_label(train) - H_label(test)|\n",
        "    deltaH = abs(float(train_world[\"H_label\"]) - float(test_world[\"H_label\"]))\n",
        "\n",
        "    # Normalise the three pieces to [0, 1] using simple caps\n",
        "    CE_MAX = 1.5   # bits\n",
        "    KL_MAX = 1.0   # bits\n",
        "    DELTAH_MAX = 1.0  # bits for a Bernoulli\n",
        "\n",
        "    ce_norm = min(ce_bits / CE_MAX, 1.0)\n",
        "    kl_norm = min(kl_bits / KL_MAX, 1.0)\n",
        "    deltaH_norm = min(deltaH / DELTAH_MAX, 1.0)\n",
        "\n",
        "    # Tension weights (sum to 1.0)\n",
        "    b_ce = 0.4\n",
        "    b_kl = 0.4\n",
        "    b_deltaH = 0.2\n",
        "\n",
        "    T_entropy = b_ce * ce_norm + b_kl * kl_norm + b_deltaH * deltaH_norm\n",
        "\n",
        "    # Effective-layer correctness: \"B-lite\" notion\n",
        "    is_correct = (acc >= 0.8) and (kl_bits <= 0.05)\n",
        "\n",
        "    return {\n",
        "        \"accuracy\": acc,\n",
        "        \"ce_bits\": ce_bits,\n",
        "        \"kl_bits\": kl_bits,\n",
        "        \"deltaH\": deltaH,\n",
        "        \"T_entropy\": float(max(0.0, min(1.0, T_entropy))),\n",
        "        \"is_correct\": bool(is_correct),\n",
        "        \"p_true1\": p_true1,\n",
        "        \"p_hat1\": p_hat1,\n",
        "    }\n",
        "\n",
        "\n",
        "# -------------------------------------------------------------------\n",
        "# 5. Cross-world evaluation and dashboard\n",
        "# -------------------------------------------------------------------\n",
        "\n",
        "rows: List[Dict[str, object]] = []\n",
        "\n",
        "for train_w in worlds:\n",
        "    for test_w in worlds:\n",
        "        res = evaluate_on_world(\n",
        "            model=models[train_w[\"name\"]],\n",
        "            train_world=train_w,\n",
        "            test_world=test_w,\n",
        "        )\n",
        "        row = {\n",
        "            \"train_world\": train_w[\"name\"],\n",
        "            \"test_world\": test_w[\"name\"],\n",
        "        }\n",
        "        row.update(res)\n",
        "        rows.append(row)\n",
        "\n",
        "df = pd.DataFrame(rows)\n",
        "\n",
        "print(\"\\nCross-world results (one row per train->test pair):\\n\")\n",
        "pd.set_option(\"display.width\", 160)\n",
        "pd.set_option(\"display.max_rows\", None)\n",
        "print(df[[\n",
        "    \"train_world\",\n",
        "    \"test_world\",\n",
        "    \"accuracy\",\n",
        "    \"ce_bits\",\n",
        "    \"kl_bits\",\n",
        "    \"deltaH\",\n",
        "    \"T_entropy\",\n",
        "    \"is_correct\",\n",
        "]].to_string(index=False))\n",
        "\n",
        "# -------------------------------------------------------------------\n",
        "# 6. Simple T_entropy heatmap\n",
        "# -------------------------------------------------------------------\n",
        "\n",
        "pivot = df.pivot(index=\"train_world\", columns=\"test_world\", values=\"T_entropy\")\n",
        "\n",
        "plt.figure(figsize=(6, 5))\n",
        "im = plt.imshow(pivot.values, origin=\"upper\", interpolation=\"nearest\")\n",
        "plt.colorbar(im, label=\"T_entropy (train -> test)\")\n",
        "\n",
        "plt.xticks(range(len(pivot.columns)), pivot.columns, rotation=45, ha=\"right\")\n",
        "plt.yticks(range(len(pivot.index)), pivot.index)\n",
        "plt.title(\"Q127-A: T_entropy for train_world -> test_world\")\n",
        "\n",
        "for i in range(pivot.shape[0]):\n",
        "    for j in range(pivot.shape[1]):\n",
        "        value = pivot.values[i, j]\n",
        "        plt.text(j, i, f\"{value:.2f}\", ha=\"center\", va=\"center\", fontsize=8)\n",
        "\n",
        "plt.tight_layout()\n",
        "plt.show()\n"
      ]
    }
  ]
}